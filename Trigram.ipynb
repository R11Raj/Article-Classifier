{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessory Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreprocess(data):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "    def removespecial(s):\n",
    "        r=list(s)\n",
    "        for i in range(len(r)):\n",
    "            if not(r[i].isalpha() or r[i].isdigit()):\n",
    "                r[i]=' '\n",
    "        return \"\".join(r)\n",
    "\n",
    "    def preprocess(s):\n",
    "        t=s.split()\n",
    "        i=0\n",
    "        while i<len(t):\n",
    "            if len(t[i])==1 and t[i] in ['s','t']:\n",
    "                t[i-1]+=t[i]\n",
    "                t.pop(i)\n",
    "            else:\n",
    "                i+=1\n",
    "        return \" \".join(t)\n",
    "\n",
    "    def removesingle(s):\n",
    "        t=s.split()\n",
    "        i=0\n",
    "        while i<len(t):\n",
    "            t[i]=t[i].lower()\n",
    "            if len(t[i])==1 and t[i] not in ['i','a']:\n",
    "                t.pop(i)\n",
    "            else:\n",
    "                i+=1\n",
    "        return \" \".join(t)\n",
    "\n",
    "    nouns_title=[]\n",
    "    nouns_content=[]\n",
    "\n",
    "    for i in data.index:  \n",
    "\n",
    "        try:\n",
    "            s=removespecial(data['Content'][i])\n",
    "            s=preprocess(s)\n",
    "            s=removesingle(s)\n",
    "            tokenized = nltk.word_tokenize(s)\n",
    "            nouns1 = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]\n",
    "\n",
    "\n",
    "            s=removespecial(data['Title'][i])\n",
    "            s=preprocess(s)\n",
    "            s=removesingle(s)\n",
    "            tokenized = nltk.word_tokenize(s)\n",
    "            nouns2 = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]\n",
    "\n",
    "            nouns_content.append(nouns1)\n",
    "            nouns_title.append(nouns2)\n",
    "        except:\n",
    "            pass\n",
    "    return nouns_title,nouns_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find Context Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findContextWords(nouns_title,nouns_content):\n",
    "  context_words=[]\n",
    "  for i in range(len(nouns_content)):\n",
    "      sim=[]\n",
    "      nouns=nouns_title[i]\n",
    "      if len(nouns)==0:\n",
    "          pass\n",
    "      else:\n",
    "          for j in nouns_content[i]:\n",
    "              s=0\n",
    "              for n in nouns:\n",
    "                  syn1 = wordnet.synsets(j)\n",
    "                  syn2 = wordnet.synsets(n)\n",
    "                  for s1 in syn1:\n",
    "                      for s2 in syn2:\n",
    "                          if s1.wup_similarity(s2)!=None:\n",
    "                              s+=s1.wup_similarity(s2)\n",
    "              if s/len(nouns)>=1:\n",
    "                  context_words.append(j)\n",
    "      print(i+1)\n",
    "  return context_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(context_words):\n",
    "  trigram={}\n",
    "  for i in range(len(context_words)-2):\n",
    "    w1,w2,w3=context_words[i:i+3]\n",
    "    trigram[(w1,w2,w3)]=trigram.get((w1,w2,w3),0)+1\n",
    "    \n",
    "  return trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To build trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_words is a list of context words derived in preprocessing stage\n",
    "with open('Context and Ngram/V1/context_words_terror.pickle', 'rb') as f:\n",
    "    terror=pickle.load(f)\n",
    "\n",
    "terror=build_trigram(terror)\n",
    "\n",
    "with open('Context and Ngram/V1/context_words_crime.pickle', 'rb') as f:\n",
    "    crime=pickle.load(f)\n",
    "\n",
    "crime=build_trigram(crime)\n",
    "\n",
    "with open('Context and Ngram/V1/context_words_cancer.pickle', 'rb') as f:\n",
    "    cancer=pickle.load(f)\n",
    "\n",
    "cancer=build_trigram(cancer)\n",
    "\n",
    "with open('Context and Ngram/V1/context_words_health.pickle', 'rb') as f:\n",
    "    health=pickle.load(f)\n",
    "\n",
    "health=build_trigram(health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_words is a list of context words derived in preprocessing stage\n",
    "with open('Context and Ngram/V2/context_words_trainterror.pickle', 'rb') as f:\n",
    "    terror=pickle.load(f)\n",
    "\n",
    "terror=build_trigram(terror)\n",
    "\n",
    "with open('Context and Ngram/V2/context_words_traincrime.pickle', 'rb') as f:\n",
    "    crime=pickle.load(f)\n",
    "\n",
    "crime=build_trigram(crime)\n",
    "\n",
    "with open('Context and Ngram/V2/context_words_traincancer.pickle', 'rb') as f:\n",
    "    cancer=pickle.load(f)\n",
    "\n",
    "cancer=build_trigram(cancer)\n",
    "\n",
    "with open('Context and Ngram/V2/context_words_trainhealth.pickle', 'rb') as f:\n",
    "    health=pickle.load(f)\n",
    "\n",
    "health=build_trigram(health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Previously build trigrams data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Context and Ngram/V1/trigram_cancer.pickle', 'rb') as f:\n",
    "    cancer=pickle.load(f)\n",
    "with open('Context and Ngram/V1/trigram_crime.pickle', 'rb') as f:\n",
    "    crime=pickle.load(f)\n",
    "with open('Context and Ngram/V1/trigram_terror.pickle', 'rb') as f:\n",
    "    terror=pickle.load(f)\n",
    "with open('Context and Ngram/V1/trigram_health.pickle', 'rb') as f:\n",
    "    health=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Context and Ngram/V2/trigram_traincancer.pickle', 'rb') as f:\n",
    "    cancer=pickle.load(f)\n",
    "with open('Context and Ngram/V2/trigram_traincrime.pickle', 'rb') as f:\n",
    "    crime=pickle.load(f)\n",
    "with open('Context and Ngram/V2/trigram_trainterror.pickle', 'rb') as f:\n",
    "    terror=pickle.load(f)\n",
    "with open('Context and Ngram/V2/trigram_trainhealth.pickle', 'rb') as f:\n",
    "    health=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BW and CW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "actual=[]\n",
    "for t in ['crime','cancer','terror','health']:\n",
    "    \n",
    "    data=pd.read_csv('Validation Data/test'+t+'.csv')\n",
    "    \n",
    "    nouns_title,nouns_content=DataPreprocess(data)\n",
    "    \n",
    "    same=0\n",
    "    for i in range(len(nouns_title)):\n",
    "        a=nouns_title[i]\n",
    "        b=nouns_content[i]\n",
    "\n",
    "        def contextwords(a,b):\n",
    "            context_words=[]\n",
    "            for j in b:\n",
    "                s=0\n",
    "                for n in a:\n",
    "                    syn1 = wordnet.synsets(j)\n",
    "                    syn2 = wordnet.synsets(n)\n",
    "                    for s1 in syn1:\n",
    "                        for s2 in syn2:\n",
    "                            if s1.wup_similarity(s2)!=None:\n",
    "                                s+=s1.wup_similarity(s2)\n",
    "                if s/len(a)>=1:\n",
    "                    context_words.append(j)\n",
    "            return context_words\n",
    "\n",
    "        context_words=contextwords(a,b)\n",
    "\n",
    "        bigram=build_trigram(context_words)\n",
    "\n",
    "        article_type={1:'crime',2:'cancer',3:'terror',4:'health'}\n",
    "\n",
    "        cw=[]\n",
    "        iter=1\n",
    "        for j in [crime,cancer,terror,health]:\n",
    "            c=0\n",
    "            bw=[]\n",
    "            for x in bigram:\n",
    "                if j.get(x,0):\n",
    "                    bw.append(bigram[x]*j[x])\n",
    "                    c+=1\n",
    "            if len(bw)==0:\n",
    "                cw.append((iter,0))\n",
    "            else:\n",
    "                f=0\n",
    "                mx=0\n",
    "                for x in bw:\n",
    "                    s=np.log(1+(c*x))\n",
    "                    if s>mx:\n",
    "                        mx=s\n",
    "                cw.append((iter,mx))\n",
    "            iter+=1\n",
    "\n",
    "\n",
    "        ans=max(cw,key=lambda x:x[1])\n",
    "        \n",
    "        preds.append(article_type[ans[0]])\n",
    "        actual.append(t)\n",
    "        \n",
    "    print(t,'Completed')  \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(actual,preds,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(actual,preds,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KESALJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1=[]\n",
    "actual1=[]\n",
    "for t in ['crime','cancer','terror','health']:\n",
    "    \n",
    "    data=pd.read_csv('Validation Data/test'+t+'.csv')\n",
    "    \n",
    "    nouns_title,nouns_content=DataPreprocess(data)\n",
    "    \n",
    "    same=0\n",
    "    for i in range(len(nouns_title)):\n",
    "        a=nouns_title[i]\n",
    "        b=nouns_content[i]\n",
    "\n",
    "        def contextwords(a,b):\n",
    "            context_words=[]\n",
    "            for j in b:\n",
    "                s=0\n",
    "                for n in a:\n",
    "                    syn1 = wordnet.synsets(j)\n",
    "                    syn2 = wordnet.synsets(n)\n",
    "                    for s1 in syn1:\n",
    "                        for s2 in syn2:\n",
    "                            if s1.wup_similarity(s2)!=None:\n",
    "                                s+=s1.wup_similarity(s2)\n",
    "                if s/len(a)>=1:\n",
    "                    context_words.append(j)\n",
    "            return context_words\n",
    "\n",
    "        context_words=contextwords(a,b)\n",
    "\n",
    "        bigram=build_trigram(context_words)\n",
    "\n",
    "        article_type={1:'crime',2:'cancer',3:'terror',4:'health'}\n",
    "        iter=1\n",
    "        kjd=[]\n",
    "        for j in [crime,cancer,terror,health]:\n",
    "            #bigrams=set(list(bigram.keys())+list(j.keys()))\n",
    "            bigrams=list(set(bigram.keys()) & set(j.keys()))\n",
    "        \n",
    "            dis=0\n",
    "            for x in bigrams:\n",
    "                a=bigram.get(x,0)\n",
    "                b=j.get(x,0)\n",
    "                dis+=(2*(b-a)/(a+b))\n",
    "            kjd.append((iter,dis))\n",
    "            iter+=1\n",
    "        \n",
    "        ans=max(kjd,key=lambda x:x[1])\n",
    "        #print(kjd)\n",
    "        preds1.append(article_type[ans[0]])\n",
    "        actual1.append(t) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(actual1,preds1,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(actual1,preds1,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BW and CW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "actual=[]\n",
    "\n",
    "data=pd.read_csv('Test Data/Finaltest.csv')\n",
    "\n",
    "nouns_title,nouns_content=DataPreprocess(data)\n",
    "actual=list(data['Query Name'])\n",
    "\n",
    "for i in range(len(nouns_title)):\n",
    "    a=nouns_title[i]\n",
    "    b=nouns_content[i]\n",
    "\n",
    "    def contextwords(a,b):\n",
    "        context_words=[]\n",
    "        for j in b:\n",
    "            s=0\n",
    "            for n in a:\n",
    "                syn1 = wordnet.synsets(j)\n",
    "                syn2 = wordnet.synsets(n)\n",
    "                for s1 in syn1:\n",
    "                    for s2 in syn2:\n",
    "                        if s1.wup_similarity(s2)!=None:\n",
    "                            s+=s1.wup_similarity(s2)\n",
    "            if len(a) and s/len(a)>=1:\n",
    "                context_words.append(j)\n",
    "        return context_words\n",
    "\n",
    "    context_words=contextwords(a,b)\n",
    "\n",
    "    bigram=build_trigram(context_words)\n",
    "\n",
    "    article_type={1:'crime',2:'cancer',3:'terror',4:'health'}\n",
    "\n",
    "    cw=[]\n",
    "    iter=1\n",
    "    for j in [crime,cancer,terror,health]:\n",
    "        c=0\n",
    "        bw=[]\n",
    "        for x in bigram:\n",
    "            if j.get(x,0):\n",
    "                bw.append(bigram[x]*j[x])\n",
    "                c+=1\n",
    "        if len(bw)==0:\n",
    "            cw.append((iter,0))\n",
    "        else:\n",
    "            f=0\n",
    "            mx=0\n",
    "            for x in bw:\n",
    "                s=np.log(1+(c*x))\n",
    "                if s>mx:\n",
    "                    mx=s\n",
    "            cw.append((iter,mx))\n",
    "        iter+=1\n",
    "\n",
    "\n",
    "    ans=max(cw,key=lambda x:x[1])\n",
    "\n",
    "    preds.append(article_type[ans[0]])\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(actual,preds,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(actual,preds,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KESALJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1=[]\n",
    "actual1=[]\n",
    "\n",
    "data=pd.read_csv('Test Data/Finaltest.csv')\n",
    "actual1=list(data['Query Name'])\n",
    "\n",
    "nouns_title,nouns_content=DataPreprocess(data)\n",
    "\n",
    "for i in range(len(nouns_title)):\n",
    "    a=nouns_title[i]\n",
    "    b=nouns_content[i]\n",
    "\n",
    "    def contextwords(a,b):\n",
    "        context_words=[]\n",
    "        for j in b:\n",
    "            s=0\n",
    "            for n in a:\n",
    "                syn1 = wordnet.synsets(j)\n",
    "                syn2 = wordnet.synsets(n)\n",
    "                for s1 in syn1:\n",
    "                    for s2 in syn2:\n",
    "                        if s1.wup_similarity(s2)!=None:\n",
    "                            s+=s1.wup_similarity(s2)\n",
    "            if len(a) and s/len(a)>=1:\n",
    "                context_words.append(j)\n",
    "        return context_words\n",
    "\n",
    "    context_words=contextwords(a,b)\n",
    "\n",
    "    bigram=build_trigram(context_words)\n",
    "\n",
    "    article_type={1:'crime',2:'cancer',3:'terror',4:'health'}\n",
    "    iter=1\n",
    "    kjd=[]\n",
    "    for j in [crime,cancer,terror,health]:\n",
    "        #bigrams=set(list(bigram.keys())+list(j.keys()))\n",
    "        bigrams=list(set(bigram.keys()) & set(j.keys()))\n",
    "        dis=0\n",
    "        for x in bigrams:\n",
    "            a=bigram.get(x,0)\n",
    "            b=j.get(x,0)\n",
    "            dis+=(2*(b-a)/(a+b))\n",
    "        kjd.append((iter,dis))\n",
    "        iter+=1\n",
    "\n",
    "    ans=max(kjd,key=lambda x:x[1])\n",
    "\n",
    "    preds1.append(article_type[ans[0]])\n",
    "print('Completed')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(actual1,preds1,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(actual1,preds1,average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
